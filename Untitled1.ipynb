{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/linhnguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linhnguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/linhnguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f67c64c7fba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import pycountry\n",
    "import pickle\n",
    "import us\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Content</th>\n",
       "      <th>Location</th>\n",
       "      <th>Username</th>\n",
       "      <th>Retweet-Count</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Created at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@ZupancicJareen Follow #BidenLies #BidenLied a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trish22758076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:59:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Link to get :ðŸ‘‡ðŸ‘‡ðŸ‘‡\\nhttps://t.co/kRoIosUS6y\\n#Tr...</td>\n",
       "      <td>Etats-Unis</td>\n",
       "      <td>Hicham21940587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:48:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>#Trump ðŸ‡ºðŸ‡¸\\nWaPo Fact-Checkers Slam #BidenLied ...</td>\n",
       "      <td>HyÃ¨res, France</td>\n",
       "      <td>C_W_UK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:47:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>YouTube doing away with dislikes bc of Whiteho...</td>\n",
       "      <td>Clown World, USA</td>\n",
       "      <td>BEcAMearekonING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:46:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Everyone knows this is happening to protect #B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BilukCyril</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:39:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                             Content          Location  \\\n",
       "0  @ZupancicJareen Follow #BidenLies #BidenLied a...               NaN   \n",
       "1  Link to get :ðŸ‘‡ðŸ‘‡ðŸ‘‡\\nhttps://t.co/kRoIosUS6y\\n#Tr...        Etats-Unis   \n",
       "2  #Trump ðŸ‡ºðŸ‡¸\\nWaPo Fact-Checkers Slam #BidenLied ...    HyÃ¨res, France   \n",
       "3  YouTube doing away with dislikes bc of Whiteho...  Clown World, USA   \n",
       "4  Everyone knows this is happening to protect #B...               NaN   \n",
       "\n",
       "          Username  Retweet-Count  Favorites           Created at  \n",
       "0    Trish22758076              0          0  2021-03-30 22:59:04  \n",
       "1   Hicham21940587              1          0  2021-03-30 22:48:48  \n",
       "2           C_W_UK              0          0  2021-03-30 22:47:54  \n",
       "3  BEcAMearekonING              0          0  2021-03-30 22:46:12  \n",
       "4       BilukCyril              0          0  2021-03-30 22:39:17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  Unnamed: 0.1  Retweet-Count     Favorites\n",
      "count  8084.000000   8084.000000    8084.000000   8084.000000\n",
      "mean   4041.500000    716.413409       2.830901     11.922934\n",
      "std    2333.794121    435.993453     117.158487    552.269555\n",
      "min       0.000000      0.000000       0.000000      0.000000\n",
      "25%    2020.750000    336.000000       0.000000      0.000000\n",
      "50%    4041.500000    691.000000       0.000000      0.000000\n",
      "75%    6062.250000   1095.000000       0.000000      2.000000\n",
      "max    8083.000000   1499.000000   10016.000000  47094.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8084 entries, 0 to 8083\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     8084 non-null   int64 \n",
      " 1   Unnamed: 0.1   8084 non-null   int64 \n",
      " 2   Content        8084 non-null   object\n",
      " 3   Location       5682 non-null   object\n",
      " 4   Username       8084 non-null   object\n",
      " 5   Retweet-Count  8084 non-null   int64 \n",
      " 6   Favorites      8084 non-null   int64 \n",
      " 7   Created at     8084 non-null   object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 505.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the output\n",
    "stimulus_raw = pd.read_csv(\"stimulus_raw.csv\")\n",
    "\n",
    "# Print the first five rows\n",
    "display(stimulus_raw.head())\n",
    "\n",
    "# Print the summary statistics\n",
    "print(stimulus_raw.describe())\n",
    "\n",
    "# Print the info\n",
    "print(stimulus_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7696 entries, 0 to 8083\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Content        7696 non-null   object        \n",
      " 1   Location       5414 non-null   object        \n",
      " 2   Username       7696 non-null   object        \n",
      " 3   Retweet-Count  7696 non-null   int64         \n",
      " 4   Favorites      7696 non-null   int64         \n",
      " 5   Created at     7696 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 420.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#We don't need the 2 columns called Unnamed, we're gonna drop them. Also, we will remove duplicated tweets. \n",
    "stimulus_raw.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop duplicated rows\n",
    "stimulus_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "# Created at column's type should be datatime\n",
    "stimulus_raw[\"Created at\"] = pd.to_datetime(stimulus_raw[\"Created at\"])\n",
    "\n",
    "# Print the info again\n",
    "print(stimulus_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[follow, bidenlies, bidenlied, bidenremorse, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[link, get, trump, fuckbiden, bidenlied, repub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[trump, wapo, slam, bidenlied, georgia, electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[youtube, away, dislikes, whitehouse, social, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[everyone, knows, happening, protect, biden, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[kidsincages, good, kidsincages, racist, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[kidsincages, good, kidsincages, racist, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[never, seen, anyone, protect, person, like, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[tired, imposed, kids, school, sick, america, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[stopaapihate, abcnews, cnn, hannity, tuckerca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[food, thoughts, stopaapihate, abcnews, cnn, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[food, thoughts, stopaapihate, abcnews, cnn, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[admitted, unpopular, president, ever, even, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[consent, vaccine, passport, iwillnotcomply, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[breaking, news, alert, bidenlied, bidenborder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[agree, disagree, newsmedia, wwe, aew, news, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[really, amazing, portland, allegedly, honoure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[putting, masks, patriotism, standing, infring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[surrounded, liars, pinocchios, nowhere, near,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[bidenlied, said, starting, scratch, fight, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Processed\n",
       "0   [follow, bidenlies, bidenlied, bidenremorse, b...\n",
       "1   [link, get, trump, fuckbiden, bidenlied, repub...\n",
       "2   [trump, wapo, slam, bidenlied, georgia, electi...\n",
       "3   [youtube, away, dislikes, whitehouse, social, ...\n",
       "4   [everyone, knows, happening, protect, biden, a...\n",
       "5   [kidsincages, good, kidsincages, racist, good,...\n",
       "6   [kidsincages, good, kidsincages, racist, good,...\n",
       "7   [never, seen, anyone, protect, person, like, b...\n",
       "8   [tired, imposed, kids, school, sick, america, ...\n",
       "9   [stopaapihate, abcnews, cnn, hannity, tuckerca...\n",
       "10  [food, thoughts, stopaapihate, abcnews, cnn, h...\n",
       "11  [food, thoughts, stopaapihate, abcnews, cnn, h...\n",
       "12  [admitted, unpopular, president, ever, even, h...\n",
       "13  [consent, vaccine, passport, iwillnotcomply, b...\n",
       "14  [breaking, news, alert, bidenlied, bidenborder...\n",
       "15  [agree, disagree, newsmedia, wwe, aew, news, b...\n",
       "16  [really, amazing, portland, allegedly, honoure...\n",
       "17  [putting, masks, patriotism, standing, infring...\n",
       "18  [surrounded, liars, pinocchios, nowhere, near,...\n",
       "19  [bidenlied, said, starting, scratch, fight, co..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we clean up the content of the tweets. We'll remove stopwords, punctuation, mention, hashtag, links, and one-or-two letter words lik a or an\n",
    "#After that, we'll tokenize the tweets\n",
    "def clean_up(tweet):\n",
    "    \n",
    "    # Remove links\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtag\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    \n",
    "    # Tokenize the words\n",
    "    tokenized = word_tokenize(tweet)\n",
    "\n",
    "    # Remove the stop words\n",
    "    tokenized = [token for token in tokenized if token not in stopwords.words(\"english\")] \n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = [lemmatizer.lemmatize(token, pos='a') for token in tokenized]\n",
    "\n",
    "    # Remove non-alphabetic characters and keep the words contains three or more letters\n",
    "    tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]\n",
    "    \n",
    "    return tokenized\n",
    "    \n",
    "# Call the function and store the result into a new column\n",
    "stimulus_raw[\"Processed\"] = stimulus_raw[\"Content\"].str.lower().apply(clean_up)\n",
    "\n",
    "\n",
    "# Print the first fifteen rows of Processed\n",
    "display(stimulus_raw[[\"Processed\"]].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>280</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td>104</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8080</th>\n",
       "      <td>155</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8081</th>\n",
       "      <td>140</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8083</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7696 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Words\n",
       "0         79      8\n",
       "1        255     24\n",
       "2        119     14\n",
       "3        278     27\n",
       "4        280     37\n",
       "...      ...    ...\n",
       "8079     104     15\n",
       "8080     155     22\n",
       "8081     140     18\n",
       "8082      52      6\n",
       "8083      60      7\n",
       "\n",
       "[7696 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We record the length of the tweets and the number of words in each tweets in new columns\n",
    "# Get the tweet lengths\n",
    "stimulus_raw[\"Length\"] = stimulus_raw[\"Content\"].str.len()\n",
    "\n",
    "# Get the number of words in tweets\n",
    "stimulus_raw[\"Words\"] = stimulus_raw[\"Content\"].str.split().str.len()\n",
    "\n",
    "# Display the new columns\n",
    "display(stimulus_raw[[\"Length\", \"Words\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: ['unknown' 'Etats-Unis' 'HyÃ¨res, France' ... 'PHL, PA' 'Kansas, USA'\n",
      " 'Tennessee']\n",
      "Unique Value count: 2043\n"
     ]
    }
   ],
   "source": [
    "#we want to record the locations of the tweet, so that maybe we can analyze how people from different regions feel about this stimulus check.\n",
    "#we'll store the location of the tweet to a column. If there's no location found, we'll mark it as 'unknown'\n",
    "stimulus_raw[\"Location\"].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Print the unique locations and number of unique locations\n",
    "print(\"Unique Values:\",stimulus_raw[\"Location\"].unique())\n",
    "print(\"Unique Value count:\",len(stimulus_raw[\"Location\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown' 'US' 'GE' 'FR' 'CU' 'CA' 'EG' 'IE' 'HK' 'BS' 'LU' 'AU' 'IL'\n",
      " 'GB' 'SG' 'MY' 'NL' 'DZ' 'GT' 'NZ' 'AQ' 'ES' 'NG' 'JM' 'DE' 'LR' 'NI'\n",
      " 'JP' 'UG' 'DK' 'CR' 'UM']\n",
      "Number of unique values: 32\n"
     ]
    }
   ],
   "source": [
    "# After recording the locations, we want to classify the locations that are states within the US versus foreign countries.\n",
    "def get_countries(location):\n",
    "    \n",
    "    # If location is a country name return its alpha2 code\n",
    "    if pycountry.countries.get(name= location):\n",
    "        return pycountry.countries.get(name = location).alpha_2\n",
    "    \n",
    "    # If location is a subdivisions name return the countries alpha2 code\n",
    "    try:\n",
    "        pycountry.subdivisions.lookup(location)\n",
    "        return pycountry.subdivisions.lookup(location).country_code\n",
    "    except:\n",
    "        # If the location is neither country nor subdivision return the \"unknown\" tag\n",
    "        return \"unknown\"\n",
    "\n",
    "# Call the function and store the country codes in the Country column\n",
    "stimulus_raw[\"Country\"] = stimulus_raw[\"Location\"].apply(get_countries)\n",
    "\n",
    "# Print the unique values\n",
    "print(stimulus_raw[\"Country\"].unique())\n",
    "\n",
    "# Print the number of unique values\n",
    "print(\"Number of unique values:\",len(stimulus_raw[\"Country\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown' 'GA' 'OK' 'ME' 'CT' 'OH' 'not-us' 'TX' 'WI' 'AR' 'FL' 'MN' 'PA'\n",
      " 'IA' 'LA' 'NY' 'MI' 'IL' 'OR' 'NJ' 'KY' 'NH' 'SC' 'MA' 'KS' 'WV' 'CA'\n",
      " 'AL' 'IN' 'CO' 'NV' 'MO' 'VA' 'DE' 'NM' 'SD' 'AZ' 'HI' 'WA' 'MD' 'NC'\n",
      " 'DC' 'MS' 'VT' 'AK' 'TN']\n",
      "Number of unique values: 46\n"
     ]
    }
   ],
   "source": [
    "def get_states(location):\n",
    "    \n",
    "    # If location is a US state name return its alpha2 code\n",
    "    if us.states.lookup(location):\n",
    "        state = us.states.lookup(location)\n",
    "        return state.abbr\n",
    "    \n",
    "    # If location is not a us state, return not-us or unknown\n",
    "    try:\n",
    "        pycountry.subdivisions.lookup(location)\n",
    "        return \"not-us\"\n",
    "    except:\n",
    "        # If the location is neither country nor subdivision return the \"unknown\" tag\n",
    "        return \"unknown\"\n",
    "\n",
    "# Call the function and store the country codes in the Country column\n",
    "stimulus_raw[\"States\"] = stimulus_raw[\"Location\"].apply(get_states)\n",
    "\n",
    "# Print the unique values\n",
    "print(stimulus_raw[\"States\"].unique())\n",
    "\n",
    "# Print the number of unique values\n",
    "print(\"Number of unique values:\",len(stimulus_raw[\"States\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Process'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Process'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-901587035532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize a Tf-idf Vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtfidf_stops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimulus_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Process\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Process'"
     ]
    }
   ],
   "source": [
    "tfidf_stops = [\"stimulus\", \"check\", \"stimulus check\", \"biden\", \"bidenlied\"]\n",
    "\n",
    "\n",
    "# Initialize a Tf-idf Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=8, stop_words= tfidf_stops)\n",
    "m=stimulus_raw[\"Process\"].apply(lambda x: ' '.join(x))\n",
    "tfidf_matrix = vectorizer.fit_transform(m)\n",
    "display(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b9b5ea61ac73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
