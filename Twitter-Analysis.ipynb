{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/linhnguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linhnguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/linhnguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.random' has no attribute 'default_rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f67c64c7fba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#: A default, shared numpy-Generator-based PRNG for any/all uses that don't require seeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mdefault_prng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.random' has no attribute 'default_rng'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import pycountry\n",
    "import pickle\n",
    "import us\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Content</th>\n",
       "      <th>Location</th>\n",
       "      <th>Username</th>\n",
       "      <th>Retweet-Count</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Created at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@ZupancicJareen Follow #BidenLies #BidenLied a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trish22758076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:59:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Link to get :ðŸ‘‡ðŸ‘‡ðŸ‘‡\\nhttps://t.co/kRoIosUS6y\\n#Tr...</td>\n",
       "      <td>Etats-Unis</td>\n",
       "      <td>Hicham21940587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:48:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>#Trump ðŸ‡ºðŸ‡¸\\nWaPo Fact-Checkers Slam #BidenLied ...</td>\n",
       "      <td>HyÃ¨res, France</td>\n",
       "      <td>C_W_UK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:47:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>YouTube doing away with dislikes bc of Whiteho...</td>\n",
       "      <td>Clown World, USA</td>\n",
       "      <td>BEcAMearekonING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:46:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Everyone knows this is happening to protect #B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BilukCyril</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-30 22:39:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                             Content          Location  \\\n",
       "0  @ZupancicJareen Follow #BidenLies #BidenLied a...               NaN   \n",
       "1  Link to get :ðŸ‘‡ðŸ‘‡ðŸ‘‡\\nhttps://t.co/kRoIosUS6y\\n#Tr...        Etats-Unis   \n",
       "2  #Trump ðŸ‡ºðŸ‡¸\\nWaPo Fact-Checkers Slam #BidenLied ...    HyÃ¨res, France   \n",
       "3  YouTube doing away with dislikes bc of Whiteho...  Clown World, USA   \n",
       "4  Everyone knows this is happening to protect #B...               NaN   \n",
       "\n",
       "          Username  Retweet-Count  Favorites           Created at  \n",
       "0    Trish22758076              0          0  2021-03-30 22:59:04  \n",
       "1   Hicham21940587              1          0  2021-03-30 22:48:48  \n",
       "2           C_W_UK              0          0  2021-03-30 22:47:54  \n",
       "3  BEcAMearekonING              0          0  2021-03-30 22:46:12  \n",
       "4       BilukCyril              0          0  2021-03-30 22:39:17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  Unnamed: 0.1  Retweet-Count     Favorites\n",
      "count  8084.000000   8084.000000    8084.000000   8084.000000\n",
      "mean   4041.500000    716.413409       2.830901     11.922934\n",
      "std    2333.794121    435.993453     117.158487    552.269555\n",
      "min       0.000000      0.000000       0.000000      0.000000\n",
      "25%    2020.750000    336.000000       0.000000      0.000000\n",
      "50%    4041.500000    691.000000       0.000000      0.000000\n",
      "75%    6062.250000   1095.000000       0.000000      2.000000\n",
      "max    8083.000000   1499.000000   10016.000000  47094.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8084 entries, 0 to 8083\n",
      "Data columns (total 8 columns):\n",
      "Unnamed: 0       8084 non-null int64\n",
      "Unnamed: 0.1     8084 non-null int64\n",
      "Content          8084 non-null object\n",
      "Location         5682 non-null object\n",
      "Username         8084 non-null object\n",
      "Retweet-Count    8084 non-null int64\n",
      "Favorites        8084 non-null int64\n",
      "Created at       8084 non-null object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 505.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the output\n",
    "stimulus_raw = pd.read_csv(\"stimulus_raw.csv\")\n",
    "\n",
    "# Print the first five rows\n",
    "display(stimulus_raw.head())\n",
    "\n",
    "# Print the summary statistics\n",
    "print(stimulus_raw.describe())\n",
    "\n",
    "# Print the info\n",
    "print(stimulus_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7696 entries, 0 to 8083\n",
      "Data columns (total 6 columns):\n",
      "Content          7696 non-null object\n",
      "Location         5414 non-null object\n",
      "Username         7696 non-null object\n",
      "Retweet-Count    7696 non-null int64\n",
      "Favorites        7696 non-null int64\n",
      "Created at       7696 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 420.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#We don't need the 2 columns called Unnamed, we're gonna drop them. Also, we will remove duplicated tweets. \n",
    "stimulus_raw.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop duplicated rows\n",
    "stimulus_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "# Created at column's type should be datatime\n",
    "stimulus_raw[\"Created at\"] = pd.to_datetime(stimulus_raw[\"Created at\"])\n",
    "\n",
    "# Print the info again\n",
    "print(stimulus_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[follow, bidenlies, bidenlied, bidenremorse, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[link, get, trump, fuckbiden, bidenlied, repub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[trump, wapo, slam, bidenlied, georgia, electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[youtube, away, dislikes, whitehouse, social, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[everyone, knows, happening, protect, biden, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[kidsincages, good, kidsincages, racist, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[kidsincages, good, kidsincages, racist, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[never, seen, anyone, protect, person, like, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[tired, imposed, kids, school, sick, america, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[stopaapihate, abcnews, cnn, hannity, tuckerca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[food, thoughts, stopaapihate, abcnews, cnn, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[food, thoughts, stopaapihate, abcnews, cnn, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[admitted, unpopular, president, ever, even, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[consent, vaccine, passport, iwillnotcomply, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[breaking, news, alert, bidenlied, bidenborder...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Processed\n",
       "0   [follow, bidenlies, bidenlied, bidenremorse, b...\n",
       "1   [link, get, trump, fuckbiden, bidenlied, repub...\n",
       "2   [trump, wapo, slam, bidenlied, georgia, electi...\n",
       "3   [youtube, away, dislikes, whitehouse, social, ...\n",
       "4   [everyone, knows, happening, protect, biden, a...\n",
       "5   [kidsincages, good, kidsincages, racist, good,...\n",
       "6   [kidsincages, good, kidsincages, racist, good,...\n",
       "7   [never, seen, anyone, protect, person, like, b...\n",
       "8   [tired, imposed, kids, school, sick, america, ...\n",
       "9   [stopaapihate, abcnews, cnn, hannity, tuckerca...\n",
       "10  [food, thoughts, stopaapihate, abcnews, cnn, h...\n",
       "11  [food, thoughts, stopaapihate, abcnews, cnn, h...\n",
       "12  [admitted, unpopular, president, ever, even, h...\n",
       "13  [consent, vaccine, passport, iwillnotcomply, b...\n",
       "14  [breaking, news, alert, bidenlied, bidenborder..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we clean up the content of the tweets. We'll remove stopwords, punctuation, mention, hashtag, links, and one-or-two letter words lik a or an\n",
    "#After that, we'll tokenize the tweets\n",
    "def clean_up(tweet):\n",
    "    \n",
    "    # Remove links\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtag\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    \n",
    "    # Tokenize the words\n",
    "    tokenized = word_tokenize(tweet)\n",
    "\n",
    "    # Remove the stop words\n",
    "    tokenized = [token for token in tokenized if token not in stopwords.words(\"english\")] \n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = [lemmatizer.lemmatize(token, pos='a') for token in tokenized]\n",
    "\n",
    "    # Remove non-alphabetic characters and keep the words contains three or more letters\n",
    "    tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]\n",
    "    \n",
    "    return tokenized\n",
    "    \n",
    "# Call the function and store the result into a new column\n",
    "stimulus_raw[\"Processed\"] = stimulus_raw[\"Content\"].str.lower().apply(clean_up)\n",
    "\n",
    "\n",
    "# Print the first fifteen rows of Processed\n",
    "display(stimulus_raw[[\"Processed\"]].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>280</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>301</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>310</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>146</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>275</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>272</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>278</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>301</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>127</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>89</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>301</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>127</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>221</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>281</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>218</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>269</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>221</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>249</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>280</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>279</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>146</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>280</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>262</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8054</th>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>104</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8056</th>\n",
       "      <td>303</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td>210</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>150</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>351</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8060</th>\n",
       "      <td>198</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>256</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8062</th>\n",
       "      <td>73</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>238</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8064</th>\n",
       "      <td>147</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8065</th>\n",
       "      <td>200</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066</th>\n",
       "      <td>119</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8067</th>\n",
       "      <td>216</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8068</th>\n",
       "      <td>89</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8069</th>\n",
       "      <td>270</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8070</th>\n",
       "      <td>266</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8071</th>\n",
       "      <td>273</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>308</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>283</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>138</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>134</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>280</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8078</th>\n",
       "      <td>124</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td>104</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8080</th>\n",
       "      <td>155</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8081</th>\n",
       "      <td>140</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8083</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7696 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Words\n",
       "0         79      8\n",
       "1        255     24\n",
       "2        119     14\n",
       "3        278     27\n",
       "4        280     37\n",
       "5        301     41\n",
       "6        310     42\n",
       "7        146     19\n",
       "8        275     45\n",
       "9        272     29\n",
       "10       278     28\n",
       "11       256     25\n",
       "12       301     34\n",
       "13        94     11\n",
       "14       127     14\n",
       "15        89     10\n",
       "16       301     41\n",
       "17       127     18\n",
       "18       221     28\n",
       "19       281     43\n",
       "20       218     29\n",
       "21       269     30\n",
       "22        78      9\n",
       "23       221     28\n",
       "24       249     37\n",
       "25       280     38\n",
       "26       279     47\n",
       "27       146     13\n",
       "28       280     53\n",
       "29       262     45\n",
       "...      ...    ...\n",
       "8054      70      8\n",
       "8055     104     14\n",
       "8056     303     41\n",
       "8057     210     35\n",
       "8058     150     24\n",
       "8059     351     54\n",
       "8060     198     34\n",
       "8061     256     43\n",
       "8062      73     11\n",
       "8063     238     40\n",
       "8064     147     20\n",
       "8065     200     39\n",
       "8066     119     15\n",
       "8067     216     39\n",
       "8068      89     17\n",
       "8069     270     48\n",
       "8070     266     45\n",
       "8071     273     40\n",
       "8072     308     54\n",
       "8073     283     46\n",
       "8074     138     19\n",
       "8075     134     23\n",
       "8076      94     17\n",
       "8077     280     41\n",
       "8078     124     20\n",
       "8079     104     15\n",
       "8080     155     22\n",
       "8081     140     18\n",
       "8082      52      6\n",
       "8083      60      7\n",
       "\n",
       "[7696 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We record the length of the tweets and the number of words in each tweets in new columns\n",
    "# Get the tweet lengths\n",
    "stimulus_raw[\"Length\"] = stimulus_raw[\"Content\"].str.len()\n",
    "\n",
    "# Get the number of words in tweets\n",
    "stimulus_raw[\"Words\"] = stimulus_raw[\"Content\"].str.split().str.len()\n",
    "\n",
    "# Display the new columns\n",
    "display(stimulus_raw[[\"Length\", \"Words\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: ['unknown' 'Etats-Unis' 'HyÃ¨res, France' ... 'PHL, PA' 'Kansas, USA'\n",
      " 'Tennessee']\n",
      "Unique Value count: 2043\n"
     ]
    }
   ],
   "source": [
    "#we want to record the locations of the tweet, so that maybe we can analyze how people from different regions feel about this stimulus check.\n",
    "#we'll store the location of the tweet to a column. If there's no location found, we'll mark it as 'unknown'\n",
    "stimulus_raw[\"Location\"].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Print the unique locations and number of unique locations\n",
    "print(\"Unique Values:\",stimulus_raw[\"Location\"].unique())\n",
    "print(\"Unique Value count:\",len(stimulus_raw[\"Location\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown' 'US' 'GE' 'FR' 'CU' 'CA' 'EG' 'IE' 'HK' 'BS' 'LU' 'AU' 'IL'\n",
      " 'GB' 'SG' 'MY' 'NL' 'DZ' 'GT' 'NZ' 'AQ' 'ES' 'NG' 'JM' 'DE' 'LR' 'NI'\n",
      " 'JP' 'UG' 'DK' 'CR' 'UM']\n",
      "Number of unique values: 32\n"
     ]
    }
   ],
   "source": [
    "# After recording the locations, we want to classify the locations that are states within the US versus foreign countries.\n",
    "def get_countries(location):\n",
    "    \n",
    "    # If location is a country name return its alpha2 code\n",
    "    if pycountry.countries.get(name= location):\n",
    "        return pycountry.countries.get(name = location).alpha_2\n",
    "    \n",
    "    # If location is a subdivisions name return the countries alpha2 code\n",
    "    try:\n",
    "        pycountry.subdivisions.lookup(location)\n",
    "        return pycountry.subdivisions.lookup(location).country_code\n",
    "    except:\n",
    "        # If the location is neither country nor subdivision return the \"unknown\" tag\n",
    "        return \"unknown\"\n",
    "\n",
    "# Call the function and store the country codes in the Country column\n",
    "stimulus_raw[\"Country\"] = stimulus_raw[\"Location\"].apply(get_countries)\n",
    "\n",
    "# Print the unique values\n",
    "print(stimulus_raw[\"Country\"].unique())\n",
    "\n",
    "# Print the number of unique values\n",
    "print(\"Number of unique values:\",len(stimulus_raw[\"Country\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown' 'GA' 'OK' 'ME' 'CT' 'OH' 'not-us' 'TX' 'WI' 'AR' 'FL' 'MN' 'PA'\n",
      " 'IA' 'LA' 'NY' 'MI' 'IL' 'OR' 'NJ' 'KY' 'NH' 'SC' 'MA' 'KS' 'WV' 'CA'\n",
      " 'AL' 'IN' 'CO' 'NV' 'MO' 'VA' 'DE' 'NM' 'SD' 'AZ' 'HI' 'WA' 'MD' 'NC'\n",
      " 'DC' 'MS' 'VT' 'AK' 'TN']\n",
      "Number of unique values: 46\n"
     ]
    }
   ],
   "source": [
    "def get_states(location):\n",
    "    \n",
    "    # If location is a US state name return its alpha2 code\n",
    "    if us.states.lookup(location):\n",
    "        state = us.states.lookup(location)\n",
    "        return state.abbr\n",
    "    \n",
    "    # If location is not a us state, return not-us or unknown\n",
    "    try:\n",
    "        pycountry.subdivisions.lookup(location)\n",
    "        return \"not-us\"\n",
    "    except:\n",
    "        # If the location is neither country nor subdivision return the \"unknown\" tag\n",
    "        return \"unknown\"\n",
    "\n",
    "# Call the function and store the country codes in the Country column\n",
    "stimulus_raw[\"States\"] = stimulus_raw[\"Location\"].apply(get_states)\n",
    "\n",
    "# Print the unique values\n",
    "print(stimulus_raw[\"States\"].unique())\n",
    "\n",
    "# Print the number of unique values\n",
    "print(\"Number of unique values:\",len(stimulus_raw[\"States\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
